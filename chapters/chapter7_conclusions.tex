\chapter{Conclusions and Future Work}
    \section{Conclusions}
    \label{conc:conc}
        % What we did:
        %   - A new heuristic approach to tackle delays in RL
        %       - From Model-Based
        %       - Using POMDP literature, in particular PSR paradigm
        %       - Applying neural density estimation for the first time
        % What we discovered:
        %   - Which are the most important parameters for the new approach
        %   - The approach is sound, Deterministic Delay in Deterministic Environment shows
        %   - However we encountered two major problems:
        %       - While L2-TRPO is able to perform well on stochastic delays, D-TRPO has structural problems
        %       - Stochastic perturbations
        
        In this thesis, we designed and implemented a new structured modular network, called Belief-Estimation module, to be deployed alongside existing Reinforcement Learning algorithm, without modifications, in order to make them able to cope with the presence of delays in the environment. We adopted the existing Model-Based approach from the DMDP literature so to model the interaction between our module and RL algorithms. The structure of the module has been inspired by POMDP literature, applying the PSR paradigm directly in a RL context. However, in light of the recent presentation of Attention networks, we decided to implement the Self-Attention mechanism contained in the Transformer architecture inside our module, as a more optimized and parallel-enabled variant of the standard Recurrent mechanisms. At last, a MAF network has been used to drive the output of the model towards a finite representation of the belief distribution of the current state, thus implementing a heuristic approach to tackle the problem of delays. By deploying the module alongside TRPO algorithm we defined a new algorithm, D-TRPO. Furthermore, an intermediate result of the research, the State-Prediction module, has been also deployed alongside TRPO constituting a baseline for our approach, L2-TRPO. \newline
        At first, we tested the module alone in order to understand which are its most important hyperparameters performance-wise, discovering that the internal dimension of the Transformer's encoder plays a very important role in allowing the module to learn a precise model of the environment. At the same time, we observed that we can afford to compress the encoder's output into a smaller vector by means of a feedforward layer, thus avoiding the curse of dimensionality, without compromising its overall performances. \newline
        Our experiments in training the two algorithms in a deterministic environment affected by deterministic delays (Section \ref{sub:res_det_delays}) has shown that the approach is sound and promising: both algorithms are able to nullify the presence of smaller delays given enough training samples, reaching the same performance TRPO can reach in an undelayed environment. For larger delays, both algorithms behave better than the other baselines. However, when training D-TRPO in the same environment affected by stochastic delays (Section \ref{sub:res_stoch_delays}), we observed inconsistent learning even with the simplest stochastic delay process. This lead us to think that the best way to cope with stochastic delays is to train the algorithm in a simulated synthetic deterministic delay setting with enough delay. This idea has been confirmed by testing D-TRPO and L2-TRPO models in environments with different amount and kind of delays: models trained in a deterministic delay setting with a sufficient amount of delay are able to reach even excellent performances when deployed in a stochastic delay setting. Unfortunately, we also observed a sharp difference from D-TRPO and L2-TRPO performances, in favor of the latter, which lead to the discover of a structural problem within the interaction between the Belief-Estimation module and TRPO algorithm. This problem hinders the capability of correctly building belief representations when the model is deployed in a context with different amount of delays, thus lowering performances of models trained in a deterministic delay settings when deployed with stochastic delays. \newline
        At last, we trained D-TRPO and L2-TRPO in a stochastic environment affected by deterministic delays, comparing the resulting performances with already trained models on the same amount of delays but in the deterministic version of the environment. Results are promising: D-TRPO shows to be less affected by the presence of stochasticity in the environment w.r.t. L2-TRPO in all tests, with a remarkable difference in half of them. This proves that the presence of the MAF network is beneficial and that the approach is sound.
        
    \section{Future Work}
    \label{conc:future}
    %   - Implement a solution that updates both Policy and Belief Module at the same time
    %       such as PSD and RPSP do. 
    %   - Test of different environments type to show strengths/weaknesses of the approach
    %   - Using the entire Transformer other than only the Encoder
    %   - Anonymous Delays applications
    %   - Repair the Belief representation distribution
        The guidelines for future works are mainly given by the flaws we discovered while testing our approach. Nevertheless, there is a series of choices and assumption that have been made during the development of the research that may be relaxed in order to advance further. To summarize, we propose the following research directions for future studies on this topic and on this research:
        \begin{itemize}
            \item Belief Representation construction: finding an upgrade to the network that makes the Belief-Estimation module output a representation of the current belief that is consistent with the one the Policy/Value networks expect, regardless of the size of the extended state. This improvement would restore performances inspected in Section \ref{sub:res_test_delays} and, specifically, it would make possibile to train D-TRPO on deterministic delays in order to deploy it in a stochastic delay setting.
            \item Conjunct optimization of modules and policies: in the presented approach, the module is introduced in the algorithm and it is optimized with its own, independent, training process. However, POMDP literature related to the PSR paradigm proposes a different setting, where the policy and the module's counterpart are optimized together through the weighted sum of their own loss functions. Solutions of this kind are to be explored.
            \item Deployment with different state-of-the-art RL algorithms: during this research we evaluated the module alongside TRPO, but its modular property makes its deployment possible with any other RL algorithm. This possibility is yet to be explored, in particular we cannot exclude that other existing algorithms may be more suitable to be trained alongside the module, possibly thanks to different properties in their Policy network updates.
            \item Anonymous delay application: during this research thesis, we assumed the amount and presence of delays are known to the Agent and design choices were made consequently. However, the general purpose is to develope an approach that is able to cope with the presence of delays in any situation and anonymous delays still have to be explored.
        \end{itemize}