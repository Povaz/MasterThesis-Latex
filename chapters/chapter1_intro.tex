\chapter{Introduction}
    Reinforcement Learning (RL) is a branch of Machine Learning concerned with modeling processes that involve an Agent interacting with an external Environment in order to complete a task. The interaction between the Agent and the Environment is modeled through the Markov Decision Process (MDP) framework: the process is divided in discrete time-step, each of them characterized by the Agent observing the current state of the Environment, executing one of the available actions and receiving a correspondent reward that evaluates its goodness. The MDP framework has been successful on numerous applications, such as autonomous driving (\pcite{intro:autonomous_driving}), industry automation (\pcite{intro:robotics}), trading in finance (\pcite{intro:finance}) and games (\pcite{intro:games} and \pcite{delay:ssbm}).
    \\\\
    However, a subtle but meaningful approximation is taken for granted by the MDP framework: instantaneousness. Infact, the MDP framework assumes that observations of the current state are immediately available, actions are immediately executed, reward is immediately perceived or, in other words, that the Environment does not change its state until the Agent provides the next action. Thus, the MDP framework implicitly assumes that the time is driven by the Agent action-selection, which by definition is discrete: whenever the agent decides the next action and executes it, the Environment immediately updates its state and generates a reward. Otherwise, the environment is paused and waiting for the Agent's action. As a consequence, any algorithm implementation based on the MDP framework directly inherits these assumptions. In reality, these assumptions are irrealistic due to the inherent physical nature of both Agent and Environment and recent studies have been focusing on the impacts of these assumption on the actual performance of the Agents, such as \pcite{delayperf:robot}, which identify three different sources of time neglected by the MDP framework:
    \begin{itemize}
        \item \textbf{Computation:} Any step that requires computation, such as selecting an action, also requires time to be completed, time in which the Environment is constantly changing. In other words, when the action is selected the state of the Environment has already changed and the action is executed in a different state. Computational time can be reduced by optimizing the Agent's algorithm, either by lowering its complexity or by exploiting potential parallel computation, but it is always present in some capacity.
        \item \textbf{Data Transmission:} Information such as a new state or a new reward is not immediately available to the Agent, since it needs to be transferred to it by means of a communication process. The mean of communication may vary significantly between different problems: an Agent may be able to perceive the environment through local sensors or may need an internet connection, which in turn provides very different range of reliability based on the physical means of connection, for example Ethernet (wired) connection or a Wi-Fi (wireless) connection.
        \item \textbf{Actuation:} Once the Agent has selected an action, its immediate execution is not granted in many cases, especially for Agents that control physical objects, such as in the Reacher Task studied by \pcite{delayperf:robot}. Infact, whenever the actions prescribe certain movements of a physical object in the Environment, actuation time is unavoidable and it is often a requirement of the task itself: whenever a certain acceleration or speed is indicated, an actuation time is implicitely defined. 
    \end{itemize}
    \noindent
    It is important to observe that neither of these three processes are linked to specific and/or rare cases, instead they are present as a property of real applications in general. In practice, their presence causes a delay in either state observation, action execution or reward perception that is not modeled by the standard MDP framework. Thus, assessing their impact in the overall interaction process between the Agent and the Environment is a key aspect: for each of them, it is important to measure how much delay they are inducing on the process w.r.t. the Environment evolution through time. Whenever their impact is not negligible, the MDP framework does not model the correspondent delay and the Agent's performances are hindered. 
    \\\\
    For these reasons, it is important to develop solutions that are able to cope with the presence of delays. However, since delays do not consistute a specific case, but rather an inherent property of reality, we would like to develop a solution that is able to tackle the problem of delays entirely, in any possible context. Furthermore, we would also like to develop a solution that can be deployed along side the current state-of-the-art algorithms for MDP problems, rather than developing a specific solution to substitute them, so that it is possible to leverage upon present and future results and performances achieved in solving MDP problems. This research thesis goal is to formulate a new structured network that can be deployed alongside any RL algorithm developed so far and in the future, which is able to provide information about the current state of the environment,regardless of the presence of delay or the type of the environment. This information is built by means of a heuristic process, which takes as input all information available to the Agent and output a finite representation of its current but unobserved position.