\chapter*{Abstract}
This research thesis is focused on sequential decision-making
problems where the interactions between agent and environment are affected by delays. Delays exist as a property of nature itself, they inherently stem either from the Agent implementation or from the real world Environment and they can greatly affect agents' capabilities and performances. In particular, delays are present in environment's state observation, agent's actions execution and reward signal perception. We work in the delayed Markov Decision Process (DMDP) framework and we consider both deterministic delays and stochastic delays in the case of both deterministic and stochastic environments. We adopt the Model-Based approach to design a new structured network, combining recent results from POMDP and Deep Learning literatures, Predictive State Representation and Self-Attention respectively, with a modular property that makes it deployable alongside existing Reinforcement Learning algorithms. The proposed network follows a heuristic approach, providing to the RL algorithm information about the position of the Agent in the form of a representation of the belief distribution of the current unobserved state of the environment. We deploy our module alongside a state-of-the-art RL algorithm in order to establish its capabilities, defining a new RL algorithm that is able to cope with deterministic delays and it is natively compatible with stochastic delays. At the end, we evaluate the effectiveness of the proposed approach against a set of chosen baselines, providing results for both deterministic and stochastic delays in both deterministic and stochastic environments, drawing our conclusions on the discovered strength and weaknesses, proposing possible future research works to mitigate the latter.