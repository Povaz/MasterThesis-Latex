\nomenclature[01]{MDP}{Markov Decision Process.}
\nomenclature[02]{$s_t$}{State of the Environment encountered by the Agent at time-step t.}
\nomenclature[03]{$a_t$}{Action chosen by the Agent at time-step t.}
\nomenclature[04]{$r_t$}{Reward collected by the Agent at time-step t.}
\nomenclature[05]{$\mathbf{S}$}{Set of all the States of the Environment or State Space.}
\nomenclature[06]{$\mathbf{A}$}{Set of all the Actions of the Environment or Action Space.}
\nomenclature[07]{$\mathbf{R}$}{Reward function.}
\nomenclature[08]{$P$}{Dynamics of the Markov Decision Process.}
\nomenclature[09]{$p$}{State-Transition Probability Function.}
\nomenclature[10]{$\pi$}{Policy Function.}
\nomenclature[11]{$\mathbf{R}_{0}^{\pi}$}{Disconted Return (starting from time-step t=0).}
\nomenclature[12]{$\mathbf{R}_{t}^{\pi}$}{Disconted Return (starting from time-step t).}
\nomenclature[13]{$v_{\pi}$}{State-Value Function.}
\nomenclature[14]{$q_{\pi}$}{Action-Value Function.}
\nomenclature[15]{$\pi^{*}$}{Optimal Policy Function.}
\nomenclature[16]{$v_{*}$}{Optimal State-Value Function.}
\nomenclature[17]{$q_{*}$}{Optimal Action-Value Function.}
\nomenclature[18]{$d_o$}{(Constant) Observation Delay.}
\nomenclature[19]{$d_a$}{(Constant) Execution Delay.}
\nomenclature[20]{$d_r$}{(Constant) Reward Delay.}
\nomenclature[21]{$d_o(t)$}{Stochastic Observation Delay.}
\nomenclature[22]{$d_a(t)$}{Stochastic Execution Delay.}
\nomenclature[23]{$d_r(t)$}{Stochastic Reward Delay.}
\nomenclature[24]{$\mathbf{O}$}{Set of all the possible Observations or Observation Space.}
\nomenclature[25]{$O$}{Observation function.}
\nomenclature[26]{$\mathbf{B}$}{Belief State.}
\nomenclature[27]{$\mathbf{R^{\pi}}$}{Policy Expected Discounted Return.}
\nomenclature[28]{$A_{\pi}$}{Advantage Function.}
\printnomenclature