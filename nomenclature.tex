\nomenclature[01]{MDP}{Markov Decision Process.}
\nomenclature[02]{$s_t$}{State of the Environment encountered by the Agent at time-step t.}
\nomenclature[03]{$a_t$}{Action chosen by the Agent at time-step t.}
\nomenclature[04]{$r_t$}{Reward collected by the Agent at time-step t.}
\nomenclature[05]{$\mathbf{S}$}{Set of all the States of the Environment or State Space.}
\nomenclature[06]{$\mathbf{A}$}{Set of all the Actions of the Environment or Action Space.}
\nomenclature[07]{$\mathbf{R}$}{Reward function.}
\nomenclature[08]{$P$}{Dynamics of the Markov Decision Process.}
\nomenclature[09]{$p$}{State-Transition Probability Function.}
\nomenclature[10]{$\pi$}{Policy Function.}
\nomenclature[11]{$\mathbf{R}_{0}^{\pi}$}{Disconted Return (starting from time-step t=0).}
\nomenclature[12]{$\mathbf{R}_{t}^{\pi}$}{Disconted Return (starting from time-step t).}
\nomenclature[13]{$v_{\pi}$}{State-Value Function.}
\nomenclature[14]{$q_{\pi}$}{Action-Value Function.}
\nomenclature[15]{$\pi^{*}$}{Optimal Policy Function.}
\nomenclature[16]{$v_{*}$}{Optimal State-Value Function.}
\nomenclature[17]{$q_{*}$}{Optimal Action-Value Function.}
\nomenclature[18]{$d_o$}{(Constant) Observation Delay.}
\nomenclature[19]{$d_a$}{(Constant) Execution Delay.}
\nomenclature[20]{$d_r$}{(Constant) Reward Delay.}
\nomenclature[21]{$d_o(t)$}{Stochastic Observation Delay.}
\nomenclature[22]{$d_a(t)$}{Stochastic Execution Delay.}
\nomenclature[23]{$d_r(t)$}{Stochastic Reward Delay.}
\nomenclature[24]{$\mathbf{O}$}{Set of all the possible Observations or Observation Space.}
\nomenclature[25]{$O$}{Observation function.}
\nomenclature[26]{$\mathbf{B}$}{Belief State.}
\nomenclature[27]{$i_t$}{Extended State experienced at time-step t.}
\nomenclature[28]{$\mathbf{I_d}$}{Augmented State Space.}
\nomenclature[29]{$\mathbf{R_d}$}{Augmented Reward Function.}
\nomenclature[30]{$\bar{s}_t$}{State Representation in the context of Model-Based approach.}
\nomenclature[31]{$\bar{p}_t$}{Approximation of a State-Transition Probability Function.}
\nomenclature[32]{$h_t$}{Internal state of a Recurrent network.}
\nomenclature[33]{$\pi_{\theta}$}{Policy parameterized w.r.t. a parameter vector $\theta$.}
\nomenclature[34]{$J$}{Performance measure of the Agent.}
\nomenclature[35]{$\mathbf{R^{\pi}}$}{Policy Expected Discounted Return.}
\nomenclature[36]{$A_{\pi}$}{Advantage Function.}
\nomenclature[37]{$D_{KL}$}{Kullback-Leibler Divergence.}
\nomenclature[38]{$\mathbf{x}$}{Input sequence.}
\nomenclature[39]{$\mathbf{c}$}{Context vector.}
\nomenclature[40]{$\mathbf{y}$}{Output sequence.}
\nomenclature[41]{$W_q$}{Query matrix in the context of Self-Attention mechanism.}
\nomenclature[42]{$W_k$}{Key matrix in the context of Self-Attention mechanism.}
\nomenclature[43]{$W_v$}{Value matrix in the context of Self-Attention mechanism.}
\nomenclature[44]{$q$}{Query vector in the context of Self-Attention mechanism.}
\nomenclature[45]{$k$}{Key vector in the context of Self-Attention mechanism.}
\nomenclature[46]{$v$}{Value vector in the context of Self-Attention mechanism.}
\nomenclature[47]{$\mathbf{z}$}{Sequence of Attention vectors.}
\nomenclature[48]{$z_i$}{Attention vector.}
\nomenclature[49]{$e$}{Number of Encoder layers in the Module.}
\nomenclature[50]{$enc_{dim}$}{Encoder vectors' dimension in the Module.}
\nomenclature[51]{$enc_{ff}$}{Feedforward layers' dimension in the Module.}


\printnomenclature